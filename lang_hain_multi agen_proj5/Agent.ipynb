{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top k = searches content char max: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11706 sha256=0a61f810327e9a539c41f8767e45143d93b29c2dbc3f548d281dedf28137d426\n",
      "  Stored in directory: c:\\users\\fazal\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200 )\n",
    "tool=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\LangChain\\Chatbot\\chatbotLLm\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\LangChain\\Chatbot\\chatbotLLm\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embed=HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "laoder=WebBaseLoader(\"https://python.langchain.com/v0.1/docs\")\n",
    "docs=laoder.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,embed)\n",
    "retriver=vectordb.as_retriever() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "langTool=create_retriever_tool(retriver,\"langsmithsearch\",\n",
    "                      \"Search for information about Langsmith.For any question about Langsmith , you must use this tool  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmithsearch'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langTool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxivNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests==2.31.0 (from arxiv)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests==2.31.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests==2.31.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests==2.31.0->arxiv) (2024.6.2)\n",
      "Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/81.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/81.1 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 30.7/81.1 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 61.4/81.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 61.4/81.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 61.4/81.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 71.7/81.1 kB 438.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.1/81.1 kB 378.9 kB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=552770aabbb4c3960fac4c1bcf5a348e3df5f65880a4a3badc0b584d83b570b5\n",
      "  Stored in directory: c:\\users\\fazal\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "Successfully installed arxiv-2.1.0 feedparser-6.0.10 requests-2.31.0 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combing all the 3 tools in list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[tool,arxiv_wrapper,langTool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "import os \n",
    "from langchain_groq import ChatGroq\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "            \n",
    "             model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from langchainhub) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\langchain\\chatbot\\chatbotllm\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.6.2)\n",
      "Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
      "Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.20 types-requests-2.32.0.20240602\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported function\n\narxiv_search=<class 'arxiv.Search'> arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>) top_k_results=1 ARXIV_MAX_QUERY_LENGTH=300 continue_on_failure=False load_max_docs=100 load_all_available_meta=False doc_content_chars_max=200 arxiv_result=<class 'arxiv.Result'>\n\nFunctions must be passed in as Dict, pydantic.BaseModel, or Callable. If they're a dict they must either be in OpenAI function format or valid JSON schema with top-level 'title' and 'description' keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_openai_tools_agent\n\u001b[1;32m----> 2\u001b[0m agent\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_openai_tools_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\LangChain\\Chatbot\\chatbotLLm\\Lib\\site-packages\\langchain\\agents\\openai_tools\\base.py:87\u001b[0m, in \u001b[0;36mcreate_openai_tools_agent\u001b[1;34m(llm, tools, prompt)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m llm_with_tools \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mbind(tools\u001b[38;5;241m=\u001b[39m[\u001b[43mconvert_to_openai_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools])\n\u001b[0;32m     89\u001b[0m agent \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     90\u001b[0m     RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m     91\u001b[0m         agent_scratchpad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: format_to_openai_tool_messages(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m|\u001b[39m OpenAIToolsAgentOutputParser()\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent\n",
      "File \u001b[1;32md:\\LangChain\\Chatbot\\chatbotLLm\\Lib\\site-packages\\langchain_core\\utils\\function_calling.py:344\u001b[0m, in \u001b[0;36mconvert_to_openai_tool\u001b[1;34m(tool)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m tool\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tool:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tool\n\u001b[1;32m--> 344\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_openai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: function}\n",
      "File \u001b[1;32md:\\LangChain\\Chatbot\\chatbotLLm\\Lib\\site-packages\\langchain_core\\utils\\function_calling.py:319\u001b[0m, in \u001b[0;36mconvert_to_openai_function\u001b[1;34m(function)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_python_function_to_openai_function(function)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported function\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunctions must be passed in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as Dict, pydantic.BaseModel, or Callable. If they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre a dict they must\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m either be in OpenAI function format or valid JSON schema with top-level\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    324\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported function\n\narxiv_search=<class 'arxiv.Search'> arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>) top_k_results=1 ARXIV_MAX_QUERY_LENGTH=300 continue_on_failure=False load_max_docs=100 load_all_available_meta=False doc_content_chars_max=200 arxiv_result=<class 'arxiv.Result'>\n\nFunctions must be passed in as Dict, pydantic.BaseModel, or Callable. If they're a dict they must either be in OpenAI function format or valid JSON schema with top-level 'title' and 'description' keys."
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
